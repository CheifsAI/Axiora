from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
import pandas as pd
import fitz  # PyMuPDF Ù„Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§Øª PDF
import os  # Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù
from DataAnalyzer import DataAnalyzer  # Importing the class from another file

FAISS_DB_PATH = "faiss_index"

# 1. ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ (PDF) Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
def load_analysis_rules_from_memory(pdf_content):
    doc = fitz.open(stream=pdf_content, filetype="pdf")
    rules = ""
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        rules += page.get_text()
    return rules

# 2. Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ«Ø§Ø¦Ù‚ (Documents) Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯
def create_documents_from_rules(rules):
    return [Document(page_content=rules)]

# 3. ØªØ¯Ø±ÙŠØ¨ Ù†Ø¸Ø§Ù… RAG Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯
def train_rag_system(documents):
    if os.path.exists(FAISS_DB_PATH):
        print("\nğŸ”„ Loading existing FAISS index...")
        vector_db = FAISS.load_local(FAISS_DB_PATH, OllamaEmbeddings(model="llama2"))
    else:
        print("\nğŸ› ï¸ Generating new embeddings and saving FAISS index...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        texts = text_splitter.split_documents(documents)
        embedding_model = OllamaEmbeddings(model="llama2")
        vector_db = FAISS.from_documents(texts, embedding_model)
        vector_db.save_local(FAISS_DB_PATH)
    
    retriever = vector_db.as_retriever(search_type="similarity", search_kwargs={"k": 3})
    llm = Ollama(model="llama2")
    prompt_template = """
    Use the following piece of context to answer the question asked.
    Please try to provide the answer only based on the context.

    {context}
    Question: {question}

    Helpful Answers:
    """
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    retrievalQA = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )
    return retrievalQA, llm

# 4. ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù CSV Ø¬Ø¯ÙŠØ¯
def load_csv(file_path):
    return pd.read_csv(file_path)

# Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
if __name__ == "__main__":
    # Load rules from PDF file
    with open("storying.pdf", "rb") as file:
        pdf_content = file.read()
    
    documents = load_analysis_rules_from_memory(pdf_content)
    
    # Train RAG model
    retrievalQA, llm = train_rag_system(documents)
    
    # Load CSV data
    df = load_csv("Regions.csv")  
    
    # Create DataAnalyzer instance
    analyzer = DataAnalyzer(df, llm=llm)
    
    # Perform data analysis
    query = analyzer.analysis_data()
    
    # Use RetrievalQA to answer the query
    result = retrievalQA.run(query)
    
    # Display the final result
    print("Final Analysis Result:")
    print(result)