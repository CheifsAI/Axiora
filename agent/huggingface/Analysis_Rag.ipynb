{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Loading existing FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Temp\\ipykernel_10620\\2270428062.py:42: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = self.retriever.get_relevant_documents(query=data_info)\n",
      "C:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Temp\\ipykernel_10620\\2270428062.py:52: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  analysis_chain = LLMChain(llm=self.llm, prompt=analysis_template)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Analysis Result:\n",
      "\n",
      "1. Key Trends and Patterns:\n",
      "Based on the provided dataset and knowledge base, we can identify several key trends and patterns in the sales data:\n",
      "\n",
      "a) Region-wise Sales Distribution: The dataset shows a clear pattern of higher sales in regions with a larger population and lower sales in rural areas. This trend is consistent across all three years, indicating that there is a strong correlation between population density and sales.\n",
      "\n",
      "b) Seasonality: There is a noticeable seasonal pattern in the data, with higher sales during the summer months (June to August) and lower sales during the winter months (December to February). This trend suggests that sales are influenced by weather patterns and seasonal events.\n",
      "\n",
      "c) Product Popularity: The dataset shows that certain products are consistently more popular than others across different regions. For example, product A is consistently the top-selling product in region 1, while product B is the top-selling product in region 2. This trend suggests that there are regional differences in consumer preferences.\n",
      "\n",
      "d) Sales Growth Rates: The dataset shows that sales growth rates vary across regions, with some regions experiencing higher growth rates than others. For example, region 3 has consistently experienced the highest sales growth rate over the past three years. This trend suggests that there are regional differences in economic conditions and consumer behavior.\n",
      "\n",
      "2. Anomalies or Outliers:\n",
      "a) Unusual Sales Patterns: The dataset shows some unusual sales patterns in regions 1 and 3, particularly during the summer months. For example, region 1 experienced a sudden increase in sales during July 2022, while region 3 experienced a sharp decrease in sales during August 2022. These patterns may indicate unexpected changes in consumer behavior or market conditions.\n",
      "\n",
      "b) Product Outliers: The dataset shows that product C has consistently low sales across all regions, despite being a popular product in other regions. This trend may indicate a supply chain issue or a lack of marketing efforts for this product.\n",
      "\n",
      "Incorporating these insights into our analysis can help us identify potential opportunities and challenges in the sales data, such as optimizing pricing strategies, adjusting marketing campaigns, or improving supply chain management.\n",
      "\n",
      "ğŸ” Answer:\n",
      "I don't know the answer to your question as there is no information provided in the given context to calculate the average sales. The context only provides a line graph with sales over time, but it doesn't provide any additional data or information to help calculate the average sales.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from OprFuncs import data_infer, extract_code, extract_questions\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM  # âœ… Updated imports\n",
    "\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, dataframe, llm, retriever):\n",
    "        self.dataframe = dataframe\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.data_info = data_infer(dataframe)\n",
    "        self.memory = []\n",
    "\n",
    "    def analysis_data(self):\n",
    "        data_info = self.data_info\n",
    "        \n",
    "        analysis_prompt = ''' \n",
    "        You are a data analyst. You have been provided with a dataset about {data_info}.\n",
    "        Here is the dataset structure:\n",
    "        {data_info}\n",
    "\n",
    "        To enhance your analysis, you have access to a knowledge base containing relevant domain knowledge, best practices, and analytical rules.\n",
    "\n",
    "        Please analyze the data by retrieving relevant insights from the knowledge base and provide a structured analysis in the following format: \n",
    "\n",
    "        1. *Key Trends and Patterns*:\n",
    "        - [Describe the key trends and patterns in the data based on both the dataset and retrieved knowledge].\n",
    "\n",
    "        2. *Anomalies or Outliers*:\n",
    "        - [Identify any anomalies or outliers in the data, incorporating relevant insights from the knowledge base].\n",
    "\n",
    "        Ensure your analysis is specific, data-driven, and incorporates retrieved domain knowledge for deeper insights.\n",
    "        '''\n",
    "\n",
    "        # Retrieve relevant knowledge from FAISS\n",
    "        retrieved_docs = self.retriever.get_relevant_documents(query=data_info)\n",
    "        retrieved_knowledge = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "        # Define the prompt template\n",
    "        analysis_template = PromptTemplate(\n",
    "            input_variables=[\"data_info\", \"retrieved_knowledge\"],\n",
    "            template=analysis_prompt\n",
    "        )\n",
    "\n",
    "        # Create a chain for analysis data\n",
    "        analysis_chain = LLMChain(llm=self.llm, prompt=analysis_template)\n",
    "\n",
    "        # Run the analysis chain with retrieved knowledge\n",
    "        analysis = analysis_chain.invoke({\"data_info\": data_info, \"retrieved_knowledge\": retrieved_knowledge})\n",
    "\n",
    "        # Ensure that `analysis` is a string before adding it to memory\n",
    "        if isinstance(analysis, dict) and \"text\" in analysis:\n",
    "            analysis = analysis[\"text\"]\n",
    "        else:\n",
    "            analysis = str(analysis)\n",
    "\n",
    "        formatted_analysis_prompt = analysis_prompt.format(data_info=data_info, retrieved_knowledge=retrieved_knowledge)\n",
    "        self.memory.append(HumanMessage(content=formatted_analysis_prompt))\n",
    "        self.memory.append(AIMessage(content=analysis))\n",
    "\n",
    "        # Return the analysis\n",
    "        return analysis \n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "\n",
    "\n",
    "# Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1ï¸âƒ£ Load rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2ï¸âƒ£ Train RAG system with FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")  # âœ… Updated class\n",
    "    \n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nğŸ”„ Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH, \n",
    "            embedding_model, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nğŸ› ï¸ Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "    \n",
    "    retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "    llm = OllamaLLM(model=\"llama2\")  # âœ… Updated class\n",
    "    \n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return retrievalQA, llm\n",
    "\n",
    "# 3ï¸âƒ£ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# ğŸš€ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"ğŸš¨ Error: The file '{pdf_file_path}' was not found!\")\n",
    "    \n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "    \n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "    \n",
    "    # Train RAG model\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "    \n",
    "    # Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"ğŸš¨ Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "    \n",
    "    # âœ… Pass retriever properly\n",
    "    analyzer = DataAnalyzer(df, llm=llm, retriever=retrievalQA.retriever)\n",
    "    \n",
    "    # Perform data analysis and generate query\n",
    "    analysis_result = analyzer.analysis_data()\n",
    "    print(\"\\nğŸ“Š Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "\n",
    "    # Send a specific question to RAG Agent\n",
    "    question = \"What is the average sales?\"\n",
    "    result = retrievalQA.invoke({\"query\": question})\n",
    "    print(\"\\nğŸ” Answer:\")\n",
    "    print(result['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Loading existing FAISS index...\n",
      "\n",
      "ğŸ“Š Analysis Result:\n",
      "1. Key Trends and Patterns:\n",
      "\n",
      "Based on both the provided dataset and the knowledge base, several trends and patterns can be observed in the sales data:\n",
      "\n",
      "* Region-wise sales performance: The dataset reveals that regions 2 and 3 have consistently outperformed region 1 in terms of sales revenue. This pattern is consistent across all years, indicating a sustained competitive advantage for these regions.\n",
      "* Seasonality: The dataset shows a clear seasonal pattern in sales, with higher sales volumes during the winter months (December to February) and lower volumes during the summer months (June to August). This trend is common across most industries and can be attributed to factors like weather, holidays, and consumer behavior.\n",
      "* Shift in market share: The dataset suggests that region 1 has experienced a decline in market share over the past two years, while regions 2 and 3 have gained significantly. This trend may indicate a change in consumer preferences or a shift in marketing strategies.\n",
      "* Diversification of product mix: The knowledge base reveals that companies in region 1 have historically focused on a limited range of products, whereas those in regions 2 and 3 have diversified their product offerings. This may indicate a competitive advantage for the latter regions.\n",
      "2. Anomalies or Outliers:\n",
      "\n",
      "Upon analyzing the dataset, several anomalies or outliers can be identified:\n",
      "\n",
      "* Region 1 sales performance: The dataset shows a significant drop in sales revenue for region 1 during the winter months (December to February) compared to previous years. This may indicate an issue with supply chain management or customer satisfaction.\n",
      "* Sales district 3: The dataset reveals that sales district 3 has consistently recorded low sales volumes despite being located in a high-demand region. Further investigation is required to identify the reasons behind this anomaly.\n",
      "* Sales_region column: The knowledge base suggests that sales_region may not be a reliable indicator of regional performance, as some regions may have inflated or deflated numbers due to factors like data entry errors or inconsistent reporting practices.\n",
      "\n",
      "In conclusion, the analysis has identified key trends and patterns in the sales data, as well as potential anomalies or outliers that require further investigation. By incorporating relevant domain knowledge and analytical rules, the analysis provides a more comprehensive understanding of the sales performance across different regions.\n",
      "\n",
      "ğŸ” Answer:\n",
      "I don't know the answer to your question as it is not provided in the context you provided. The text only mentions the total sales figure for a particular time period, but does not provide any information on the average sales.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from OprFuncs import data_infer, extract_code, extract_questions\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, dataframe, llm, retriever):\n",
    "        self.dataframe = dataframe\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.data_info = data_infer(dataframe)\n",
    "        self.memory = []\n",
    "\n",
    "    def analysis_data(self):\n",
    "        data_info = self.data_info\n",
    "        \n",
    "        analysis_prompt = ''' \n",
    "        You are a data analyst. You have been provided with a dataset about {data_info}.\n",
    "        Here is the dataset structure:\n",
    "        {data_info}\n",
    "\n",
    "        To enhance your analysis, you have access to a knowledge base containing relevant domain knowledge, best practices, and analytical rules.\n",
    "\n",
    "        Please analyze the data by retrieving relevant insights from the knowledge base and provide a structured analysis in the following format: \n",
    "\n",
    "        1. *Key Trends and Patterns*:\n",
    "        - [Describe the key trends and patterns in the data based on both the dataset and retrieved knowledge].\n",
    "\n",
    "        2. *Anomalies or Outliers*:\n",
    "        - [Identify any anomalies or outliers in the data, incorporating relevant insights from the knowledge base].\n",
    "\n",
    "        Ensure your analysis is specific, data-driven, and incorporates retrieved domain knowledge for deeper insights.\n",
    "        '''\n",
    "\n",
    "        # Retrieve relevant knowledge from FAISS\n",
    "        retrieved_docs = self.retriever.invoke(input=data_info)  # Fixed: Use `input` instead of `query`\n",
    "        retrieved_knowledge = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "        # Define the prompt template\n",
    "        analysis_template = PromptTemplate(\n",
    "            input_variables=[\"data_info\", \"retrieved_knowledge\"],\n",
    "            template=analysis_prompt\n",
    "        )\n",
    "\n",
    "        # Create a chain for analysis data using RunnableSequence\n",
    "        analysis_chain = analysis_template | self.llm  # Updated approach\n",
    "\n",
    "        # Run the analysis chain with retrieved knowledge\n",
    "        analysis = analysis_chain.invoke({\"data_info\": data_info, \"retrieved_knowledge\": retrieved_knowledge})\n",
    "\n",
    "        # Ensure that `analysis` is a string before adding it to memory\n",
    "        if isinstance(analysis, dict) and \"text\" in analysis:\n",
    "            analysis = analysis[\"text\"]\n",
    "        else:\n",
    "            analysis = str(analysis)\n",
    "\n",
    "        formatted_analysis_prompt = analysis_prompt.format(data_info=data_info, retrieved_knowledge=retrieved_knowledge)\n",
    "        self.memory.append(HumanMessage(content=formatted_analysis_prompt))\n",
    "        self.memory.append(AIMessage(content=analysis))\n",
    "\n",
    "        # Return the analysis\n",
    "        return analysis \n",
    "\n",
    "\n",
    "# Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1ï¸âƒ£ Load rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2ï¸âƒ£ Train RAG system with FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "    \n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nğŸ”„ Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH, \n",
    "            embedding_model, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nğŸ› ï¸ Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "    \n",
    "    retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "    llm = OllamaLLM(model=\"llama2\")\n",
    "    \n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return retrievalQA, llm\n",
    "\n",
    "# 3ï¸âƒ£ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# ğŸš€ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"ğŸš¨ Error: The file '{pdf_file_path}' was not found!\")\n",
    "    \n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "    \n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "    \n",
    "    # Train RAG model\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "    \n",
    "    # Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"ğŸš¨ Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "    \n",
    "    # âœ… Pass retriever properly\n",
    "    analyzer = DataAnalyzer(df, llm=llm, retriever=retrievalQA.retriever)\n",
    "    \n",
    "    # Perform data analysis and generate query\n",
    "    analysis_result = analyzer.analysis_data()\n",
    "    print(\"\\nğŸ“Š Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "\n",
    "    # Send a specific question to RAG Agent\n",
    "    question = \"What is the average sales?\"\n",
    "    result = retrievalQA.invoke({\"query\": question})\n",
    "    print(\"\\nğŸ” Answer:\")\n",
    "    print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LLMChain\nconfig\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'temperature': 0.3, 'max_tokens': 500}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\u001b[39;00m\n\u001b[0;32m     36\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m DataAnalyzer(llm\u001b[38;5;241m=\u001b[39mrag_agent, dataframe\u001b[38;5;241m=\u001b[39mdf)\n\u001b[1;32m---> 37\u001b[0m data_insights \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalysis_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Ø§Ø³ØªØ¹Ù„Ø§Ù… ØªÙØ§Ø¹Ù„ÙŠ Ù„ÙˆØ¶Ø¹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©\u001b[39;00m\n\u001b[0;32m     40\u001b[0m query_with_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBased on the following data insights: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_insights\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, apply the rules from the document to generate insights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\My-Githup\\Axiora\\agent\\huggingface\\DataAnalyzer.py:53\u001b[0m, in \u001b[0;36mDataAnalyzer.analysis_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m analysis_template \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m     48\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_info\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     49\u001b[0m     template\u001b[38;5;241m=\u001b[39manalysis_prompt\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Create a chain for analysis data\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m analysis_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manalysis_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Fixed parameter\u001b[39;49;00m\n\u001b[0;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Run the analysis chain on the provided data\u001b[39;00m\n\u001b[0;32m     60\u001b[0m analysis_result \u001b[38;5;241m=\u001b[39m analysis_chain\u001b[38;5;241m.\u001b[39mrun(data_info\u001b[38;5;241m=\u001b[39mdata_info)\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     emit_warning()\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for LLMChain\nconfig\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'temperature': 0.3, 'max_tokens': 500}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOllama\n",
    "import pandas as pd \n",
    "from DataAnalyzer import DataAnalyzer\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ ÙˆØªÙ‚Ø³ÙŠÙ…Ù‡\n",
    "pdf_loader = PyPDFLoader(\"storying.pdf\")\n",
    "documents = pdf_loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "# Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS\n",
    "vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "# Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "# ØªØ­ÙˆÙŠÙ„ RAG Ø¥Ù„Ù‰ Agent Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ConversationalRetrievalChain\n",
    "rag_agent = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)\n",
    "\n",
    "df = pd.read_csv(\"Regions.csv\")\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "analyzer = DataAnalyzer(llm=rag_agent, dataframe=df)\n",
    "data_insights = analyzer.analysis_data()\n",
    "\n",
    "# Ø§Ø³ØªØ¹Ù„Ø§Ù… ØªÙØ§Ø¹Ù„ÙŠ Ù„ÙˆØ¶Ø¹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©\n",
    "query_with_context = f\"Based on the following data insights: {data_insights}, apply the rules from the document to generate insights.\"\n",
    "result = rag_agent.invoke({\"question\": query_with_context, \"chat_history\": []})\n",
    "\n",
    "# Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªÙŠØ¬Ø©\n",
    "print(\"\\nğŸ” Final Analysis Result:\")\n",
    "print(f\"ğŸ“Š Data Insights: \\n{data_insights}\\n\")\n",
    "print(f\"ğŸ“– Rule-based Insights: \\n{result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ConversationalRetrievalChain\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "from DataAnalyzer import DataAnalyzer  # Importing DataAnalyzer\n",
    "\n",
    "# ğŸ”¹ Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1ï¸âƒ£ Load analysis rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2ï¸âƒ£ Train RAG system and set up FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "\n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nğŸ”„ Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nğŸ› ï¸ Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "\n",
    "    retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "    llm = Ollama(model=\"llama2\")\n",
    "\n",
    "    # ğŸ”¹ Create an interactive Agent using ConversationalRetrievalChain\n",
    "    rag_agent = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)\n",
    "\n",
    "    return rag_agent, llm\n",
    "\n",
    "# 3ï¸âƒ£ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# ğŸš€ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # ğŸ”¹ Load analysis rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"ğŸš¨ Error: The file '{pdf_file_path}' was not found!\")\n",
    "\n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "\n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "\n",
    "    # ğŸ”¹ Train RAG model and create an Agent\n",
    "    rag_agent, llm = train_rag_system(documents)\n",
    "\n",
    "    # ğŸ”¹ Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"ğŸš¨ Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "\n",
    "    # ğŸ”¹ Create a DataAnalyzer instance and analyze data\n",
    "    analyzer = DataAnalyzer(df, llm=rag_agent)\n",
    "    analysis_result, questions = analyzer.analysis_data()\n",
    "\n",
    "    print(\"\\nğŸ“Š Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "\n",
    "    # ğŸ”¹ Pass extracted questions to the Agent for answers\n",
    "    for question in questions:\n",
    "        print(f\"\\nâ“ Question: {question}\")\n",
    "        result = rag_agent.invoke({\"question\": question})\n",
    "        print(\"ğŸ” Answer:\")\n",
    "        print(result['answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
