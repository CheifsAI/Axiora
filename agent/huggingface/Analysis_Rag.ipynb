{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Temp\\ipykernel_15180\\553736750.py:24: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
      "C:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Temp\\ipykernel_15180\\553736750.py:41: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama2\")\n",
      "d:\\My-Githup\\Axiora\\agent\\huggingface\\DataAnalyzer.py:53: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  analysis_chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Loading existing FAISS index...\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LLMChain\nconfig\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'temperature': 0.3, 'max_tokens': 500}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m DataAnalyzer(df, llm\u001b[38;5;241m=\u001b[39mllm)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Perform data analysis and generate query\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m analysis_result, questions \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalysis_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📊 Analysis Result:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(analysis_result)\n",
      "File \u001b[1;32md:\\My-Githup\\Axiora\\agent\\huggingface\\DataAnalyzer.py:53\u001b[0m, in \u001b[0;36mDataAnalyzer.analysis_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m analysis_template \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m     48\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_info\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     49\u001b[0m     template\u001b[38;5;241m=\u001b[39manalysis_prompt\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Create a chain for analysis data\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m analysis_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manalysis_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Fixed parameter\u001b[39;49;00m\n\u001b[0;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Run the analysis chain on the provided data\u001b[39;00m\n\u001b[0;32m     60\u001b[0m analysis_result \u001b[38;5;241m=\u001b[39m analysis_chain\u001b[38;5;241m.\u001b[39mrun(data_info\u001b[38;5;241m=\u001b[39mdata_info)\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     emit_warning()\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for LLMChain\nconfig\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'temperature': 0.3, 'max_tokens': 500}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "from DataAnalyzer import DataAnalyzer  # استيراد DataAnalyzer\n",
    "\n",
    "# Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1️⃣ Load rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2️⃣ Train RAG system with FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "    \n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\n🔄 Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH, \n",
    "            embedding_model, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n🛠️ Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "    \n",
    "    retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "    llm = Ollama(model=\"llama2\")\n",
    "    \n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return retrievalQA, llm\n",
    "\n",
    "# 3️⃣ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# 🚀 Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"🚨 Error: The file '{pdf_file_path}' was not found!\")\n",
    "    \n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "    \n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "    \n",
    "    # Train RAG model\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "    \n",
    "    # Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"🚨 Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "    \n",
    "    # Create DataAnalyzer instance\n",
    "    analyzer = DataAnalyzer(df, llm=llm)\n",
    "    \n",
    "    # Perform data analysis and generate query\n",
    "    analysis_result, questions = analyzer.analysis_data()\n",
    "    print(\"\\n📊 Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "    \n",
    "    # Send each question to RAG Agent\n",
    "    for question in questions:\n",
    "        print(f\"\\n❓ Question: {question}\")\n",
    "        result = retrievalQA.invoke({\"query\": question})\n",
    "        print(\"🔍 Answer:\")\n",
    "        print(result['result'])\n",
    "        print(\"\\n📚 Source Documents:\")\n",
    "        for doc in result['source_documents']:\n",
    "            print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LLMChain\nconfig\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'temperature': 0.3, 'max_tokens': 500}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# تحميل وتحليل البيانات\u001b[39;00m\n\u001b[0;32m     36\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m DataAnalyzer(llm\u001b[38;5;241m=\u001b[39mrag_agent, dataframe\u001b[38;5;241m=\u001b[39mdf)\n\u001b[1;32m---> 37\u001b[0m data_insights \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalysis_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# استعلام تفاعلي لوضع تحليل البيانات في سياق المعلومات المسترجعة\u001b[39;00m\n\u001b[0;32m     40\u001b[0m query_with_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBased on the following data insights: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_insights\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, apply the rules from the document to generate insights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\My-Githup\\Axiora\\agent\\huggingface\\DataAnalyzer.py:53\u001b[0m, in \u001b[0;36mDataAnalyzer.analysis_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m analysis_template \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m     48\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_info\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     49\u001b[0m     template\u001b[38;5;241m=\u001b[39manalysis_prompt\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Create a chain for analysis data\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m analysis_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manalysis_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Fixed parameter\u001b[39;49;00m\n\u001b[0;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Run the analysis chain on the provided data\u001b[39;00m\n\u001b[0;32m     60\u001b[0m analysis_result \u001b[38;5;241m=\u001b[39m analysis_chain\u001b[38;5;241m.\u001b[39mrun(data_info\u001b[38;5;241m=\u001b[39mdata_info)\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     emit_warning()\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for LLMChain\nconfig\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'temperature': 0.3, 'max_tokens': 500}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOllama\n",
    "import pandas as pd \n",
    "from DataAnalyzer import DataAnalyzer\n",
    "\n",
    "# تحميل المستند وتقسيمه\n",
    "pdf_loader = PyPDFLoader(\"storying.pdf\")\n",
    "documents = pdf_loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "# إنشاء قاعدة بيانات FAISS\n",
    "vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "# إنشاء نموذج المحادثة\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "# تحويل RAG إلى Agent باستخدام ConversationalRetrievalChain\n",
    "rag_agent = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)\n",
    "\n",
    "df = pd.read_csv(\"Regions.csv\")\n",
    "\n",
    "# تحميل وتحليل البيانات\n",
    "analyzer = DataAnalyzer(llm=rag_agent, dataframe=df)\n",
    "data_insights = analyzer.analysis_data()\n",
    "\n",
    "# استعلام تفاعلي لوضع تحليل البيانات في سياق المعلومات المسترجعة\n",
    "query_with_context = f\"Based on the following data insights: {data_insights}, apply the rules from the document to generate insights.\"\n",
    "result = rag_agent.invoke({\"question\": query_with_context, \"chat_history\": []})\n",
    "\n",
    "# إخراج النتيجة\n",
    "print(\"\\n🔍 Final Analysis Result:\")\n",
    "print(f\"📊 Data Insights: \\n{data_insights}\\n\")\n",
    "print(f\"📖 Rule-based Insights: \\n{result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ConversationalRetrievalChain\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "from DataAnalyzer import DataAnalyzer  # Importing DataAnalyzer\n",
    "\n",
    "# 🔹 Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1️⃣ Load analysis rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2️⃣ Train RAG system and set up FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "\n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\n🔄 Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n🛠️ Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "\n",
    "    retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "    llm = Ollama(model=\"llama2\")\n",
    "\n",
    "    # 🔹 Create an interactive Agent using ConversationalRetrievalChain\n",
    "    rag_agent = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)\n",
    "\n",
    "    return rag_agent, llm\n",
    "\n",
    "# 3️⃣ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# 🚀 Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # 🔹 Load analysis rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"🚨 Error: The file '{pdf_file_path}' was not found!\")\n",
    "\n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "\n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "\n",
    "    # 🔹 Train RAG model and create an Agent\n",
    "    rag_agent, llm = train_rag_system(documents)\n",
    "\n",
    "    # 🔹 Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"🚨 Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "\n",
    "    # 🔹 Create a DataAnalyzer instance and analyze data\n",
    "    analyzer = DataAnalyzer(df, llm=llm)\n",
    "    analysis_result, questions = analyzer.analysis_data()\n",
    "\n",
    "    print(\"\\n📊 Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "\n",
    "    # 🔹 Pass extracted questions to the Agent for answers\n",
    "    for question in questions:\n",
    "        print(f\"\\n❓ Question: {question}\")\n",
    "        result = rag_agent.invoke({\"question\": question})\n",
    "        print(\"🔍 Answer:\")\n",
    "        print(result['answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
