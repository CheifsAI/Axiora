{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Loading existing FAISS index...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m DataAnalyzer(df, llm\u001b[38;5;241m=\u001b[39mllm)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Perform data analysis and generate query\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m analysis_result, questions \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39manalysis_data()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìä Analysis Result:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(analysis_result)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "from DataAnalyzer import DataAnalyzer  # ÿßÿ≥ÿ™Ÿäÿ±ÿßÿØ DataAnalyzer\n",
    "\n",
    "# Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1Ô∏è‚É£ Load rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2Ô∏è‚É£ Train RAG system with FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "    \n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nüîÑ Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH, \n",
    "            embedding_model, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nüõ†Ô∏è Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "    \n",
    "    retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "    llm = Ollama(model=\"llama2\")\n",
    "    \n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return retrievalQA, llm\n",
    "\n",
    "# 3Ô∏è‚É£ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# üöÄ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{pdf_file_path}' was not found!\")\n",
    "    \n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "    \n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "    \n",
    "    # Train RAG model\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "    \n",
    "    # Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "    \n",
    "    # Create DataAnalyzer instance\n",
    "    analyzer = DataAnalyzer(df, llm=llm)\n",
    "    \n",
    "    # Perform data analysis and generate query\n",
    "    analysis_result, questions = analyzer.analysis_data()\n",
    "    print(\"\\nüìä Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "    \n",
    "    # Send each question to RAG Agent\n",
    "    for question in questions:\n",
    "        print(f\"\\n‚ùì Question: {question}\")\n",
    "        result = retrievalQA.invoke({\"query\": question})\n",
    "        print(\"üîç Answer:\")\n",
    "        print(result['result'])\n",
    "        print(\"\\nüìö Source Documents:\")\n",
    "        for doc in result['source_documents']:\n",
    "            print(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
