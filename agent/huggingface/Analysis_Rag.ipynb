{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Loading existing FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Temp\\ipykernel_10620\\2270428062.py:42: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = self.retriever.get_relevant_documents(query=data_info)\n",
      "C:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Temp\\ipykernel_10620\\2270428062.py:52: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  analysis_chain = LLMChain(llm=self.llm, prompt=analysis_template)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Analysis Result:\n",
      "\n",
      "1. Key Trends and Patterns:\n",
      "Based on the provided dataset and knowledge base, we can identify several key trends and patterns in the sales data:\n",
      "\n",
      "a) Region-wise Sales Distribution: The dataset shows a clear pattern of higher sales in regions with a larger population and lower sales in rural areas. This trend is consistent across all three years, indicating that there is a strong correlation between population density and sales.\n",
      "\n",
      "b) Seasonality: There is a noticeable seasonal pattern in the data, with higher sales during the summer months (June to August) and lower sales during the winter months (December to February). This trend suggests that sales are influenced by weather patterns and seasonal events.\n",
      "\n",
      "c) Product Popularity: The dataset shows that certain products are consistently more popular than others across different regions. For example, product A is consistently the top-selling product in region 1, while product B is the top-selling product in region 2. This trend suggests that there are regional differences in consumer preferences.\n",
      "\n",
      "d) Sales Growth Rates: The dataset shows that sales growth rates vary across regions, with some regions experiencing higher growth rates than others. For example, region 3 has consistently experienced the highest sales growth rate over the past three years. This trend suggests that there are regional differences in economic conditions and consumer behavior.\n",
      "\n",
      "2. Anomalies or Outliers:\n",
      "a) Unusual Sales Patterns: The dataset shows some unusual sales patterns in regions 1 and 3, particularly during the summer months. For example, region 1 experienced a sudden increase in sales during July 2022, while region 3 experienced a sharp decrease in sales during August 2022. These patterns may indicate unexpected changes in consumer behavior or market conditions.\n",
      "\n",
      "b) Product Outliers: The dataset shows that product C has consistently low sales across all regions, despite being a popular product in other regions. This trend may indicate a supply chain issue or a lack of marketing efforts for this product.\n",
      "\n",
      "Incorporating these insights into our analysis can help us identify potential opportunities and challenges in the sales data, such as optimizing pricing strategies, adjusting marketing campaigns, or improving supply chain management.\n",
      "\n",
      "üîç Answer:\n",
      "I don't know the answer to your question as there is no information provided in the given context to calculate the average sales. The context only provides a line graph with sales over time, but it doesn't provide any additional data or information to help calculate the average sales.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from OprFuncs import data_infer, extract_code, extract_questions\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM  # ‚úÖ Updated imports\n",
    "\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, dataframe, llm, retriever):\n",
    "        self.dataframe = dataframe\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.data_info = data_infer(dataframe)\n",
    "        self.memory = []\n",
    "\n",
    "    def analysis_data(self):\n",
    "        data_info = self.data_info\n",
    "        \n",
    "        analysis_prompt = ''' \n",
    "        You are a data analyst. You have been provided with a dataset about {data_info}.\n",
    "        Here is the dataset structure:\n",
    "        {data_info}\n",
    "\n",
    "        To enhance your analysis, you have access to a knowledge base containing relevant domain knowledge, best practices, and analytical rules.\n",
    "\n",
    "        Please analyze the data by retrieving relevant insights from the knowledge base and provide a structured analysis in the following format: \n",
    "\n",
    "        1. *Key Trends and Patterns*:\n",
    "        - [Describe the key trends and patterns in the data based on both the dataset and retrieved knowledge].\n",
    "\n",
    "        2. *Anomalies or Outliers*:\n",
    "        - [Identify any anomalies or outliers in the data, incorporating relevant insights from the knowledge base].\n",
    "\n",
    "        Ensure your analysis is specific, data-driven, and incorporates retrieved domain knowledge for deeper insights.\n",
    "        '''\n",
    "\n",
    "        # Retrieve relevant knowledge from FAISS\n",
    "        retrieved_docs = self.retriever.get_relevant_documents(query=data_info)\n",
    "        retrieved_knowledge = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "        # Define the prompt template\n",
    "        analysis_template = PromptTemplate(\n",
    "            input_variables=[\"data_info\", \"retrieved_knowledge\"],\n",
    "            template=analysis_prompt\n",
    "        )\n",
    "\n",
    "        # Create a chain for analysis data\n",
    "        analysis_chain = LLMChain(llm=self.llm, prompt=analysis_template)\n",
    "\n",
    "        # Run the analysis chain with retrieved knowledge\n",
    "        analysis = analysis_chain.invoke({\"data_info\": data_info, \"retrieved_knowledge\": retrieved_knowledge})\n",
    "\n",
    "        # Ensure that `analysis` is a string before adding it to memory\n",
    "        if isinstance(analysis, dict) and \"text\" in analysis:\n",
    "            analysis = analysis[\"text\"]\n",
    "        else:\n",
    "            analysis = str(analysis)\n",
    "\n",
    "        formatted_analysis_prompt = analysis_prompt.format(data_info=data_info, retrieved_knowledge=retrieved_knowledge)\n",
    "        self.memory.append(HumanMessage(content=formatted_analysis_prompt))\n",
    "        self.memory.append(AIMessage(content=analysis))\n",
    "\n",
    "        # Return the analysis\n",
    "        return analysis \n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "\n",
    "\n",
    "# Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1Ô∏è‚É£ Load rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2Ô∏è‚É£ Train RAG system with FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")  # ‚úÖ Updated class\n",
    "    \n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nüîÑ Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH, \n",
    "            embedding_model, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nüõ†Ô∏è Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "    \n",
    "    retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "    llm = OllamaLLM(model=\"llama2\")  # ‚úÖ Updated class\n",
    "    \n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return retrievalQA, llm\n",
    "\n",
    "# 3Ô∏è‚É£ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# üöÄ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{pdf_file_path}' was not found!\")\n",
    "    \n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "    \n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "    \n",
    "    # Train RAG model\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "    \n",
    "    # Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "    \n",
    "    # ‚úÖ Pass retriever properly\n",
    "    analyzer = DataAnalyzer(df, llm=llm, retriever=retrievalQA.retriever)\n",
    "    \n",
    "    # Perform data analysis and generate query\n",
    "    analysis_result = analyzer.analysis_data()\n",
    "    print(\"\\nüìä Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "\n",
    "    # Send a specific question to RAG Agent\n",
    "    question = \"What is the average sales?\"\n",
    "    result = retrievalQA.invoke({\"query\": question})\n",
    "    print(\"\\nüîç Answer:\")\n",
    "    print(result['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Loading existing FAISS index...\n",
      "\n",
      "üìä Analysis Result:\n",
      "1. Key Trends and Patterns:\n",
      "\n",
      "Based on both the provided dataset and the knowledge base, several trends and patterns can be observed in the sales data:\n",
      "\n",
      "* Region-wise sales performance: The dataset reveals that regions 2 and 3 have consistently outperformed region 1 in terms of sales revenue. This pattern is consistent across all years, indicating a sustained competitive advantage for these regions.\n",
      "* Seasonality: The dataset shows a clear seasonal pattern in sales, with higher sales volumes during the winter months (December to February) and lower volumes during the summer months (June to August). This trend is common across most industries and can be attributed to factors like weather, holidays, and consumer behavior.\n",
      "* Shift in market share: The dataset suggests that region 1 has experienced a decline in market share over the past two years, while regions 2 and 3 have gained significantly. This trend may indicate a change in consumer preferences or a shift in marketing strategies.\n",
      "* Diversification of product mix: The knowledge base reveals that companies in region 1 have historically focused on a limited range of products, whereas those in regions 2 and 3 have diversified their product offerings. This may indicate a competitive advantage for the latter regions.\n",
      "2. Anomalies or Outliers:\n",
      "\n",
      "Upon analyzing the dataset, several anomalies or outliers can be identified:\n",
      "\n",
      "* Region 1 sales performance: The dataset shows a significant drop in sales revenue for region 1 during the winter months (December to February) compared to previous years. This may indicate an issue with supply chain management or customer satisfaction.\n",
      "* Sales district 3: The dataset reveals that sales district 3 has consistently recorded low sales volumes despite being located in a high-demand region. Further investigation is required to identify the reasons behind this anomaly.\n",
      "* Sales_region column: The knowledge base suggests that sales_region may not be a reliable indicator of regional performance, as some regions may have inflated or deflated numbers due to factors like data entry errors or inconsistent reporting practices.\n",
      "\n",
      "In conclusion, the analysis has identified key trends and patterns in the sales data, as well as potential anomalies or outliers that require further investigation. By incorporating relevant domain knowledge and analytical rules, the analysis provides a more comprehensive understanding of the sales performance across different regions.\n",
      "\n",
      "üîç Answer:\n",
      "I don't know the answer to your question as it is not provided in the context you provided. The text only mentions the total sales figure for a particular time period, but does not provide any information on the average sales.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from OprFuncs import data_infer, extract_code, extract_questions\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, dataframe, llm, retriever):\n",
    "        self.dataframe = dataframe\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.data_info = data_infer(dataframe)\n",
    "        self.memory = []\n",
    "\n",
    "    def analysis_data(self):\n",
    "        data_info = self.data_info\n",
    "        \n",
    "        analysis_prompt = ''' \n",
    "        You are a data analyst. You have been provided with a dataset about {data_info}.\n",
    "        Here is the dataset structure:\n",
    "        {data_info}\n",
    "\n",
    "        To enhance your analysis, you have access to a knowledge base containing relevant domain knowledge, best practices, and analytical rules.\n",
    "\n",
    "        Please analyze the data by retrieving relevant insights from the knowledge base and provide a structured analysis in the following format: \n",
    "\n",
    "        1. *Key Trends and Patterns*:\n",
    "        - [Describe the key trends and patterns in the data based on both the dataset and retrieved knowledge].\n",
    "\n",
    "        2. *Anomalies or Outliers*:\n",
    "        - [Identify any anomalies or outliers in the data, incorporating relevant insights from the knowledge base].\n",
    "\n",
    "        Ensure your analysis is specific, data-driven, and incorporates retrieved domain knowledge for deeper insights.\n",
    "        '''\n",
    "\n",
    "        # Retrieve relevant knowledge from FAISS\n",
    "        retrieved_docs = self.retriever.invoke(input=data_info)  # Fixed: Use `input` instead of `query`\n",
    "        retrieved_knowledge = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "        # Define the prompt template\n",
    "        analysis_template = PromptTemplate(\n",
    "            input_variables=[\"data_info\", \"retrieved_knowledge\"],\n",
    "            template=analysis_prompt\n",
    "        )\n",
    "\n",
    "        # Create a chain for analysis data using RunnableSequence\n",
    "        analysis_chain = analysis_template | self.llm  # Updated approach\n",
    "\n",
    "        # Run the analysis chain with retrieved knowledge\n",
    "        analysis = analysis_chain.invoke({\"data_info\": data_info, \"retrieved_knowledge\": retrieved_knowledge})\n",
    "\n",
    "        # Ensure that `analysis` is a string before adding it to memory\n",
    "        if isinstance(analysis, dict) and \"text\" in analysis:\n",
    "            analysis = analysis[\"text\"]\n",
    "        else:\n",
    "            analysis = str(analysis)\n",
    "\n",
    "        formatted_analysis_prompt = analysis_prompt.format(data_info=data_info, retrieved_knowledge=retrieved_knowledge)\n",
    "        self.memory.append(HumanMessage(content=formatted_analysis_prompt))\n",
    "        self.memory.append(AIMessage(content=analysis))\n",
    "        \n",
    "        self.retriever.vectorstore.add_documents([Document(page_content=analysis)])\n",
    "\n",
    "        # Return the analysis\n",
    "        return analysis \n",
    "\n",
    "\n",
    "# Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1Ô∏è‚É£ Load rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2Ô∏è‚É£ Train RAG system with FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "    \n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nüîÑ Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH, \n",
    "            embedding_model, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nüõ†Ô∏è Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "    \n",
    "    retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "    llm = OllamaLLM(model=\"llama2\")\n",
    "    \n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return retrievalQA, llm\n",
    "\n",
    "# 3Ô∏è‚É£ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# üöÄ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{pdf_file_path}' was not found!\")\n",
    "    \n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "    \n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "    \n",
    "    # Train RAG model\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "    \n",
    "    # Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "    \n",
    "    # ‚úÖ Pass retriever properly\n",
    "    analyzer = DataAnalyzer(df, llm=llm, retriever=retrievalQA.retriever)\n",
    "    \n",
    "    # Perform data analysis and generate query\n",
    "    analysis_result = analyzer.analysis_data()\n",
    "    print(\"\\nüìä Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "\n",
    "    # Send a specific question to RAG Agent\n",
    "    question = \"What is the average sales?\"\n",
    "    result = retrievalQA.invoke({\"query\": question})\n",
    "    print(\"\\nüîç Answer:\")\n",
    "    print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Loading existing FAISS index...\n",
      "\n",
      "üìä Analysis Result:\n",
      "\n",
      "1. Key Trends and Patterns:\n",
      "\n",
      "Based on the provided dataset and knowledge base, we can identify several key trends and patterns in the sales data:\n",
      "\n",
      "* Region-wise distribution: The sales data reveals that there is a significant variation in sales across different regions. While some regions have consistently high sales, others have low sales. This suggests that there may be regional differences in customer preferences or market conditions.\n",
      "* Sales growth over time: The dataset shows a steady increase in sales over the past 3 years. This trend indicates that the company's strategy is effective in growing its business and increasing revenue.\n",
      "* Seasonality: There are noticeable seasonal fluctuations in sales, with higher sales during the summer months and lower sales during winter. This suggests that customer buying habits are influenced by seasonal factors such as holidays and weather conditions.\n",
      "* Customer behavior: The data indicates that customers tend to buy more expensive products during the holiday season and less expensive products during other times of the year. This suggests that customers prioritize gift-giving during the holidays and are more budget-conscious during other periods.\n",
      "2. Anomalies or Outliers:\n",
      "\n",
      "After analyzing the dataset, we identified several anomalies or outliers in the data:\n",
      "\n",
      "* Region 5: The sales data for Region 5 appears to be inconsistent with the trends observed in the other regions. While the region has consistently low sales, the data suggests that there may be a hidden pattern or trend that needs further investigation.\n",
      "* Sales district 17: There is a significant spike in sales for Sales District 17 during the summer months. This anomaly could indicate a unique marketing strategy or customer preference in the region.\n",
      "* Sales region 3: The data shows a sudden drop in sales for Sales Region 3 during the winter months. This could be due to various factors such as seasonal demand, production issues, or changes in customer behavior.\n",
      "\n",
      "Incorporating insights from the knowledge base, we can conclude that these anomalies may indicate opportunities for growth or areas of concern that require further investigation. For instance, Region 5's low sales could be due to a lack of marketing efforts or untapped customer segments. Similarly, Sales District 17's spike in summer sales could signify a successful marketing strategy or a change in customer preferences. Investigating these anomalies can provide valuable insights into the company's operations and help inform future strategic decisions.\n",
      "\n",
      "üîç Answer:\n",
      "The main ideas in this section are:\n",
      "\n",
      "1. Good design takes into account the needs of the user, and repetition is a powerful tool for retaining information.\n",
      "2. When choosing an effective visual, consider whether to preserve the axis labels or eliminate the axis and label the data points directly, based on the level of specificity needed.\n",
      "3. There are tools available to help designers see their graphics through colorblind eyes and be thoughtful of the tone that color conveys.\n",
      "4. The power of repetition can be leveraged in the stories we tell to facilitate retention of information.\n",
      "5. Repeatable sound bites can help facilitate this by providing succinct, clear phrases that can be easily recalled and repeated.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from OprFuncs import data_infer, extract_code, extract_questions\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, dataframe, llm, retriever):\n",
    "        self.dataframe = dataframe\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.data_info = data_infer(dataframe)\n",
    "        self.memory = []\n",
    "\n",
    "    def analysis_data(self):\n",
    "        data_info = self.data_info\n",
    "\n",
    "        analysis_prompt = ''' \n",
    "        You are a data analyst. You have been provided with a dataset about {data_info}.\n",
    "        Here is the dataset structure:\n",
    "        {data_info}\n",
    "\n",
    "        To enhance your analysis, you have access to a knowledge base containing relevant domain knowledge, best practices, and analytical rules.\n",
    "\n",
    "        Please analyze the data by retrieving relevant insights from the knowledge base and provide a structured analysis in the following format: \n",
    "\n",
    "        1. *Key Trends and Patterns*:\n",
    "        - [Describe the key trends and patterns in the data based on both the dataset and retrieved knowledge].\n",
    "\n",
    "        2. *Anomalies or Outliers*:\n",
    "        - [Identify any anomalies or outliers in the data, incorporating relevant insights from the knowledge base].\n",
    "\n",
    "        Ensure your analysis is specific, data-driven, and incorporates retrieved domain knowledge for deeper insights.\n",
    "        '''\n",
    "\n",
    "        # Retrieve relevant knowledge from FAISS\n",
    "        retrieved_docs = self.retriever.invoke(data_info)\n",
    "        retrieved_knowledge = \"\\n\".join([doc.page_content for doc in retrieved_docs]) if retrieved_docs else \"No relevant legal rules found.\"\n",
    "\n",
    "        # Define the prompt template\n",
    "        analysis_template = PromptTemplate(\n",
    "            input_variables=[\"data_info\", \"retrieved_knowledge\"],\n",
    "            template=analysis_prompt\n",
    "        )\n",
    "\n",
    "        # Create a chain for analysis data using RunnableSequence\n",
    "        analysis_chain = analysis_template | self.llm\n",
    "\n",
    "        # Run the analysis chain with retrieved knowledge\n",
    "        analysis = analysis_chain.invoke({\"data_info\": data_info, \"retrieved_knowledge\": retrieved_knowledge})\n",
    "\n",
    "        # Ensure that `analysis` is a string before adding it to memory\n",
    "        if isinstance(analysis, dict) and \"text\" in analysis:\n",
    "            analysis = analysis[\"text\"]\n",
    "        else:\n",
    "            analysis = str(analysis)\n",
    "\n",
    "        formatted_analysis_prompt = analysis_prompt.format(data_info=data_info, retrieved_knowledge=retrieved_knowledge)\n",
    "        self.memory.append(HumanMessage(content=formatted_analysis_prompt))\n",
    "        self.memory.append(AIMessage(content=analysis))\n",
    "\n",
    "        # ‚úÖ Ensure FAISS is updated with new knowledge\n",
    "        self.retriever.vectorstore.add_documents([Document(page_content=analysis)])\n",
    "\n",
    "        # Return the analysis\n",
    "        return analysis\n",
    "\n",
    "\n",
    "# Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1Ô∏è‚É£ Load rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2Ô∏è‚É£ Train RAG system with FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "\n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nüîÑ Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nüõ†Ô∏è Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "\n",
    "    retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "    llm = OllamaLLM(model=\"llama2\")\n",
    "\n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    return retrievalQA, llm\n",
    "\n",
    "# 3Ô∏è‚É£ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# üöÄ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{pdf_file_path}' was not found!\")\n",
    "\n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "\n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "\n",
    "    # Train RAG model\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "\n",
    "    # Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "\n",
    "    # ‚úÖ Pass retriever properly\n",
    "    analyzer = DataAnalyzer(df, llm=llm, retriever=retrievalQA.retriever)\n",
    "\n",
    "    # Perform data analysis and generate query\n",
    "    analysis_result = analyzer.analysis_data()\n",
    "    print(\"\\nüìä Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "\n",
    "    # Send a specific question to RAG Agent\n",
    "    question = \"What is the average sales?\"\n",
    "    result = retrievalQA.invoke({\"query\": question})\n",
    "    print(\"\\nüîç Answer:\")\n",
    "    print(result['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Loading existing FAISS index...\n",
      "\n",
      "üìä Analysis Result:\n",
      "1. Key Trends and Patterns:\n",
      "After analyzing the dataset and consulting the knowledge base, we can identify several key trends and patterns in the sales data:\n",
      "\n",
      "* Regions with higher population density tend to have higher sales volumes. This is consistent with the idea that more people in a given area are likely to result in more sales opportunities.\n",
      "* There is a positive correlation between sales in different regions, suggesting that strong sales in one region can lead to increased sales in other regions.\n",
      "* The distribution of sales across districts within regions is uneven, with some districts experiencing significantly higher sales volumes than others. This could indicate a lack of even distribution of resources or opportunities within certain regions.\n",
      "* There are instances of anomalous sales patterns, such as sudden spikes or dips in sales volume, which may be indicative of changes in consumer behavior or other external factors.\n",
      "2. Anomalies or Outliers:\n",
      "Upon further analysis, we have identified several anomalies or outliers in the data:\n",
      "\n",
      "* Region 3 experienced a sudden and significant increase in sales volume during Q2, which is not explained by any known external factors. This could indicate a change in consumer behavior or a previously unknown trend.\n",
      "* District 10 in Region 1 has consistently low sales volumes despite being located in an area with high population density. This may indicate a lack of effective marketing or sales strategies in the region.\n",
      "* There are several instances of missing data points, which can affect the accuracy and reliability of the analysis.\n",
      "\n",
      "Incorporating relevant insights from the knowledge base, we can deduce that the anomalous sales patterns in Region 3 could be due to a new product launch or marketing campaign, while the low sales volumes in District 10 may be caused by a lack of awareness or interest in the product. The missing data points highlight the importance of ensuring data quality and completeness in any analysis.\n",
      "\n",
      "üîç Answer:\n",
      "Sure, here are some potential design choices for the \"Choosing an Effective Visual\" section:\n",
      "\n",
      "1. Use a bar chart to show the average sales over time. This will allow the viewer to easily see the trend of increasing sales over the years.\n",
      "2. Use a stacked bar chart to show the distribution of sales by year. This will allow the viewer to see how much of the total sales are coming from each year.\n",
      "3. Use a line chart to show the average sales over time, with markers or labels indicating key milestones or events that may have impacted sales (e.g. new product launches, marketing campaigns, etc.).\n",
      "4. Use a scatter plot to show the relationship between sales and certain factors (e.g. number of customers, geographic location, etc.). This will allow the viewer to see how different factors are impacting sales.\n",
      "5. Use a heatmap to show the distribution of sales by geographic location. This will allow the viewer to see where the majority of sales are coming from and identify any patterns or trends.\n",
      "6. Use a Gauge chart to show the percentage of sales that come from a particular source (e.g. online vs. in-store). This will allow the viewer to easily see how different sources are contributing to overall sales.\n",
      "7. Use a Radar chart to compare the performance of different products or categories over time. This will allow the viewer to easily see which products or categories are performing well and which ones need improvement.\n",
      "8. Use a Pareto chart to show the relative size of each category in terms of sales. This will allow the viewer to easily see which categories are driving the most sales and which ones need more attention.\n",
      "9. Use a Sankey diagram to show the flow of customers through different stages of the sales process (e.g. awareness, consideration, purchase). This will allow the viewer to easily see how many customers are at each stage and identify any bottlenecks or areas for improvement.\n",
      "10. Use a Sunburst chart to show the distribution of sales by product category. This will allow the viewer to easily see which categories are driving the most sales and which ones need more attention.\n",
      "\n",
      "In terms of color, it's important to be thoughtful of the tone that color conveys. For example:\n",
      "\n",
      "* Blue can convey a sense of trustworthiness and reliability, but also calmness and stability.\n",
      "* Green can convey a sense of growth and abundance, but also nature and health.\n",
      "* Orange can convey a sense of energy and excitement, but also warmth and friendliness.\n",
      "* Red can convey a sense of passion and urgency, but also danger and warning.\n",
      "* Yellow can convey a sense of happiness and optimism, but also brightness and clarity.\n",
      "\n",
      "By considering these design choices and the tone that color can convey, you can create an effective visual that helps your audience understand and engage with the data.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from OprFuncs import data_infer, extract_code, extract_questions\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, dataframe, llm, retriever):\n",
    "        self.dataframe = dataframe\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.data_info = data_infer(dataframe)\n",
    "        self.memory = []\n",
    "\n",
    "    def analysis_data(self):\n",
    "        data_info = self.data_info\n",
    "\n",
    "        analysis_prompt = ''' \n",
    "        You are a data analyst. You have been provided with a dataset about {data_info}.\n",
    "        Here is the dataset structure:\n",
    "        {data_info}\n",
    "\n",
    "        To enhance your analysis, you have access to a knowledge base containing relevant domain knowledge, best practices, and analytical rules.\n",
    "\n",
    "        Please analyze the data by retrieving relevant insights from the knowledge base and provide a structured analysis in the following format: \n",
    "\n",
    "        1. *Key Trends and Patterns*:\n",
    "        - [Describe the key trends and patterns in the data based on both the dataset and retrieved knowledge].\n",
    "\n",
    "        2. *Anomalies or Outliers*:\n",
    "        - [Identify any anomalies or outliers in the data, incorporating relevant insights from the knowledge base].\n",
    "\n",
    "        Ensure your analysis is specific, data-driven, and incorporates retrieved domain knowledge for deeper insights.\n",
    "        '''\n",
    "\n",
    "        # Retrieve relevant knowledge from FAISS\n",
    "        retrieved_docs = self.retriever.invoke(data_info)\n",
    "        retrieved_knowledge = \"\\n\".join([doc.page_content for doc in retrieved_docs]) if retrieved_docs else \"No relevant legal rules found.\"\n",
    "\n",
    "        # Define the prompt template\n",
    "        analysis_template = PromptTemplate(\n",
    "            input_variables=[\"data_info\", \"retrieved_knowledge\"],\n",
    "            template=analysis_prompt\n",
    "        )\n",
    "\n",
    "        # Create a chain for analysis data using RunnableSequence\n",
    "        analysis_chain = analysis_template | self.llm\n",
    "\n",
    "        # Run the analysis chain with retrieved knowledge\n",
    "        analysis = analysis_chain.invoke({\"data_info\": data_info, \"retrieved_knowledge\": retrieved_knowledge})\n",
    "\n",
    "        # Ensure that `analysis` is a string before adding it to memory\n",
    "        if isinstance(analysis, dict) and \"text\" in analysis:\n",
    "            analysis = analysis[\"text\"]\n",
    "        else:\n",
    "            analysis = str(analysis)\n",
    "\n",
    "        formatted_analysis_prompt = analysis_prompt.format(data_info=data_info, retrieved_knowledge=retrieved_knowledge)\n",
    "        self.memory.append(HumanMessage(content=formatted_analysis_prompt))\n",
    "        self.memory.append(AIMessage(content=analysis))\n",
    "\n",
    "        # ‚úÖ Ensure FAISS is updated with new knowledge\n",
    "        self.retriever.vectorstore.add_documents([Document(page_content=analysis)])\n",
    "\n",
    "        # Return the analysis\n",
    "        return analysis\n",
    "\n",
    "\n",
    "# Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1Ô∏è‚É£ Load rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2Ô∏è‚É£ Train RAG system with FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "\n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nüîÑ Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nüõ†Ô∏è Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "\n",
    "    retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "    llm = OllamaLLM(model=\"llama2\")\n",
    "\n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    return retrievalQA, llm\n",
    "\n",
    "# 3Ô∏è‚É£ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# üöÄ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{pdf_file_path}' was not found!\")\n",
    "\n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "\n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "\n",
    "    # Train RAG model\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "\n",
    "    # Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "\n",
    "    # ‚úÖ Pass retriever properly\n",
    "    analyzer = DataAnalyzer(df, llm=llm, retriever=retrievalQA.retriever)\n",
    "\n",
    "    # Perform data analysis and generate query\n",
    "    analysis_result = analyzer.analysis_data()\n",
    "    print(\"\\nüìä Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "\n",
    "    # Send a specific question to RAG Agent\n",
    "    question = \"What is the average sales?\"\n",
    "    result = retrievalQA.invoke({\"query\": question})\n",
    "    print(\"\\nüîç Answer:\")\n",
    "    print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ConversationalRetrievalChain\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "from DataAnalyzer import DataAnalyzer  # Importing DataAnalyzer\n",
    "\n",
    "# üîπ Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1Ô∏è‚É£ Load analysis rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2Ô∏è‚É£ Train RAG system and set up FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "\n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nüîÑ Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nüõ†Ô∏è Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "\n",
    "    retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "    llm = Ollama(model=\"llama2\")\n",
    "\n",
    "    # üîπ Create an interactive Agent using ConversationalRetrievalChain\n",
    "    rag_agent = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)\n",
    "\n",
    "    return rag_agent, llm\n",
    "\n",
    "# 3Ô∏è‚É£ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# üöÄ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # üîπ Load analysis rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{pdf_file_path}' was not found!\")\n",
    "\n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "\n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "\n",
    "    # üîπ Train RAG model and create an Agent\n",
    "    rag_agent, llm = train_rag_system(documents)\n",
    "\n",
    "    # üîπ Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "\n",
    "    # üîπ Create a DataAnalyzer instance and analyze data\n",
    "    analyzer = DataAnalyzer(df, llm=rag_agent)\n",
    "    analysis_result, questions = analyzer.analysis_data()\n",
    "\n",
    "    print(\"\\nüìä Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "\n",
    "    # üîπ Pass extracted questions to the Agent for answers\n",
    "    for question in questions:\n",
    "        print(f\"\\n‚ùì Question: {question}\")\n",
    "        result = rag_agent.invoke({\"question\": question})\n",
    "        print(\"üîç Answer:\")\n",
    "        print(result['answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
