{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain, RetrievalQA\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Temp\\ipykernel_13656\\3629795972.py:12: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding_model = OllamaEmbeddings(model=\"llama3.2:3b\")\n"
     ]
    }
   ],
   "source": [
    "# Load PDFs from directory\n",
    "loader = PyPDFDirectoryLoader(\"./data\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_split = text_splitter.split_documents(docs)\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "\n",
    "# Initialize the vector database\n",
    "vector_db = FAISS.from_documents(text_split, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Embedding using Huggingface\n",
    "# generating text embeddings\n",
    "huggingface_embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", # generating high-quality embeddings for English text\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(huggingface_embedding.embed_query(text_split[0].page_content))\n",
    "np.array(huggingface_embedding.embed_query(text_split[0].page_content)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vector database\n",
    "db1 = FAISS.from_documents(text_split[:120], huggingface_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 U.S. Census Bureau\\nWHAT IS HEALTH INSURANCE COVERAGE?\\nThis brief presents state-level estimates of health insurance coverage \\nusing data from the American Community Survey (ACS). The  \\nU.S. Census Bureau conducts the ACS throughout the year; the \\nsurvey asks respondents to report their coverage at the time of \\ninterview. The resulting measure of health insurance coverage, \\ntherefore, reflects an annual average of current comprehensive \\nhealth insurance coverage status.* This uninsured rate measures a \\ndifferent concept than the measure based on the Current Population \\nSurvey Annual Social and Economic Supplement (CPS ASEC). \\nFor reporting purposes, the ACS broadly classifies health insurance \\ncoverage as private insurance or public insurance. The ACS defines \\nprivate health insurance as a plan provided through an employer \\nor a union, coverage purchased directly by an individual from an \\ninsurance company or through an exchange (such as healthcare.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the vector database\n",
    "query = \"WHAT IS HEALTH INSURANCE COVERAGE?\"\n",
    "\n",
    "search = db1.similarity_search(query)\n",
    "search[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000027C69C1F140>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the retriever\n",
    "retriever = db1.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceHub\n  Value error, Did not find huggingfacehub_api_token, please add an environment variable `HUGGINGFACEHUB_API_TOKEN` which contains it, or pass `huggingfacehub_api_token` as a named parameter. [type=value_error, input_value={'repo_id': 'mistralai/Mi...acehub_api_token': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceHub\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize the HuggingFaceHub model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m hf\u001b[38;5;241m=\u001b[39m\u001b[43mHuggingFaceHub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistralai/Mistral-7B-v0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the health insurance coverage?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m hf\u001b[38;5;241m.\u001b[39minvoke(query)\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     emit_warning()\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for HuggingFaceHub\n  Value error, Did not find huggingfacehub_api_token, please add an environment variable `HUGGINGFACEHUB_API_TOKEN` which contains it, or pass `huggingfacehub_api_token` as a named parameter. [type=value_error, input_value={'repo_id': 'mistralai/Mi...acehub_api_token': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "# Initialize the HuggingFaceHub model\n",
    "hf=HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_kwargs={\"temperature\":0.1,\"max_length\":500}\n",
    ")\n",
    "\n",
    "query=\"What is the health insurance coverage?\"\n",
    "hf.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following piece of context to answer the question asked.\n",
    "Please try to provide the answer only based on the context\n",
    "\n",
    "{context}\n",
    "Question:{question}\n",
    "\n",
    "Helpful Answers:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\",\"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RetrievalQA chain\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query\n",
    "query = \"DIFFERENCES IN THE UNINSURED RATE BY STATE IN 2022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use the following piece of context to answer the question asked.\n",
      "Please try to provide the answer only based on the context\n",
      "\n",
      "comparison of ACS and CPS ASEC measures \n",
      "of health insurance coverage, refer to < www.\n",
      "census.gov/topics/health/health-insurance/\n",
      "guidance.html >.\n",
      "9 Respondents may have more than one \n",
      "health insurance coverage type at the time \n",
      "of interview. As a result, adding the total \n",
      "number of people with private coverage and \n",
      "the total number with public coverage will \n",
      "sum to more than the total number with any \n",
      "coverage.‚Ä¢ From 2021 to 2022, nine states \n",
      "reported increases in private \n",
      "coverage, while seven reported \n",
      "decreases (Appendix Table B-2). \n",
      "DIFFERENCES IN THE \n",
      "UNINSURED RATE BY STATE \n",
      "IN 2022\n",
      "In 2022, uninsured rates at the \n",
      "time of interview ranged across \n",
      "states from a low of 2.4 percent \n",
      "in Massachusetts to a high of 16.6 \n",
      "percent in Texas, compared to the \n",
      "national rate of 8.0 percent.10 Ten \n",
      "of the 15 states with uninsured \n",
      "10 The uninsured rates in the District \n",
      "of Columbia and Massachusetts were not \n",
      "statistically different.rates above the national aver -\n",
      "\n",
      "percent (Appendix Table B-5). \n",
      "Medicaid coverage accounted \n",
      "for a portion of that difference. \n",
      "Medicaid coverage was 22.7 per -\n",
      "cent in the group of states that \n",
      "expanded Medicaid eligibility and \n",
      "18.0 percent in the group of nonex -\n",
      "pansion states.\n",
      "CHANGES IN THE UNINSURED \n",
      "RATE BY STATE FROM 2021 \n",
      "TO 2022\n",
      "From 2021 to 2022, uninsured rates \n",
      "decreased across 27 states, while \n",
      "only Maine had an increase. The \n",
      "uninsured rate in Maine increased \n",
      "from 5.7 percent to 6.6 percent, \n",
      "although it remained below the \n",
      "national average. Maine‚Äôs uninsured \n",
      "rate was still below 8.0 percent, \n",
      "21 Douglas Conway and Breauna Branch, \n",
      "‚ÄúHealth Insurance Coverage Status and Type \n",
      "by Geography: 2019 and 2021,‚Äù 2022, < www.\n",
      "census.gov/content/dam/Census/library/\n",
      "publications/2022/acs/acsbr-013.pdf >.\n",
      "\n",
      "library/publications/2022/acs/acsbr-013.pdf >.\n",
      "39 In 2022, the private coverage rates were \n",
      "not statistically different in North Dakota and \n",
      "Utah.Figure /five.tab/period.tab\n",
      "Percentage of Uninsured People for the /two.tab/five.tab Most Populous Metropolitan \n",
      "Areas/colon.tab /two.tab/zero.tab/two.tab/one.tab and /two.tab/zero.tab/two.tab/two.tab\n",
      "(Civilian, noninstitutionalized population) /uni00A0\n",
      "* Denotes a statistically signiÔ¨Åcant change between 2021 and 2022 at the 90 percent conÔ¨Ådence level.\n",
      "Note: For information on conÔ¨Ådentiality protection, sampling error, nonsampling error, and deÔ¨Ånitions in the American Community\n",
      "Survey, refer to <https://www2.census.gov/programs-surveys/acs/tech_docs/accuracy/ACS_Accuracy_of_Data_2022.pdf>.\n",
      "Source: U.S. Census Bureau, 2021 and 2022 American Community Survey, 1-year estimates. Boston-Cambridge-Newton/comma.tab MA-NH\n",
      "San Francisco-Oakland-Berkeley/comma.tab CA\n",
      "*Detroit-Warren-Dearborn/comma.tab MI\n",
      "Question:DIFFERENCES IN THE UNINSURED RATE BY STATE IN 2022\n",
      "\n",
      "Helpful Answers:\n",
      "- The uninsured rate in Massachusetts was 2.4% in 2022.\n",
      "- The uninsured rate in Texas was 16.6% in 2022.\n",
      "- The national uninsured rate was 8.0% in 2022.\n",
      "- Ten states had uninsured rates above the national average of 8.0% in 2022.\n",
      "- Medicaid coverage was higher in states that expanded Medicaid eligibility (22.7%) compared to nonexpansion states (18.0%).\n",
      "- Maine was the only state with an increase in the uninsured rate from 2021 to 2022, increasing from 5.7% to 6.6%.\n",
      "- The private coverage rates in North Dakota and Utah were not statistically different in 2022.\n",
      "- The uninsured rates for the most populous metropolitan areas in 2022 were: Boston-Cambridge-Newton, MA-NH (3.5%), San Francisco-Oakland-Berkeley, CA (4.6%), and Detroit-Warren-Dearborn, MI (13.2%).\n"
     ]
    }
   ],
   "source": [
    "# Call the QA chain with our query.\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use the following piece of context to answer the question asked.\n",
      "Please try to provide the answer only based on the context\n",
      "\n",
      "region_id: 19 | sales_district: Seattle | sales_region: North West\n",
      "\n",
      "region_id: 23 | sales_district: Salem | sales_region: North West\n",
      "\n",
      "region_id: 22 | sales_district: Portland | sales_region: North West\n",
      "Question: \n",
      "        You are a data analyst. You are provided with a dataset about <class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 109 entries, 0 to 108\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   region_id       109 non-null    int64 \n",
      " 1   sales_district  109 non-null    object\n",
      " 2   sales_region    109 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.7+ KB\n",
      "\n",
      "        Here is the dataset structure:\n",
      "        <class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 109 entries, 0 to 108\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   region_id       109 non-null    int64 \n",
      " 1   sales_district  109 non-null    object\n",
      " 2   sales_region    109 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.7+ KB\n",
      "\n",
      "\n",
      "        Please analyze the data and provide insights about:\n",
      "        1. Key trends and patterns in the <class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 109 entries, 0 to 108\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   region_id       109 non-null    int64 \n",
      " 1   sales_district  109 non-null    object\n",
      " 2   sales_region    109 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.7+ KB\n",
      ".\n",
      "        2. Any anomalies or outliers in the data.\n",
      "        3. Recommendations or actionable insights based on the analyzed data.\n",
      "         \"\"\"\n",
      "\n",
      "        # 1. Key trends and patterns in the data\n",
      "        # - The dataset contains 109 entries with 3 columns: region_id, sales_district, and sales_region.\n",
      "        # - The region_id column is of integer data type, while sales_district and sales_region are of object data type.\n",
      "        # - There are no missing values in the dataset.\n",
      "\n",
      "        # 2. Anomalies or outliers in the data\n",
      "        # - Since the dataset is small and does not contain any numerical data, it is difficult to identify anomalies or outliers.\n",
      "        # - However, if there are any duplicate entries or inconsistent data in the sales_district and sales_region columns, they should be investigated.\n",
      "\n",
      "        # 3. Recommendations or actionable insights based on the analyzed data\n",
      "        # - To gain more insights from the data, additional data columns could be added, such as sales amount, number of customers, or product categories.\n",
      "        # - Analyzing the sales_district and sales_region columns can provide insights into the distribution of sales across different regions and districts.\n",
      "        # - Further analysis can be conducted to identify the top-performing regions or districts, as well as any trends or patterns in sales performance.\n",
      "\n",
      "Helpful Answers:\n",
      "1. Key trends and patterns in the data:\n",
      "   - All entries in the dataset belong to the North West sales region.\n",
      "   - The sales_district column has three unique values: Seattle, Salem, and Portland.\n",
      "   - The region_id column has three unique values: 19, 22, and 23.\n",
      "\n",
      "2. Anomalies or outliers in the data:\n",
      "   - There are no apparent anomalies or outliers in the given dataset, as all the data seems to be consistent and valid.\n",
      "\n",
      "3. Recommendations or actionable insights based on the analyzed data:\n",
      "   - To gain more insights, consider adding more columns to the dataset, such as sales amount, number of customers, or product categories.\n",
      "   - Analyze the sales performance for each sales_district within the North West sales_region to identify top-performing districts.\n",
      "   - Investigate any trends or patterns in sales performance over time, if historical data is available.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain, RetrievalQA\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from DataAnalyzer import DataAnalyzer\n",
    "import os\n",
    "\n",
    "# Set your Hugging Face API token\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"Regions.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create LangChain Document objects\n",
    "documents = [\n",
    "    Document(page_content=\" | \".join([f\"{col}: {str(row[col])}\" for col in df.columns]))\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_split = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "\n",
    "# Initialize the vector database\n",
    "vector_db = FAISS.from_documents(text_split, embedding_model)\n",
    "\n",
    "# Embedding using Huggingface\n",
    "huggingface_embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",  # High-quality embeddings for English text\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Create the FAISS vector store\n",
    "db1 = FAISS.from_documents(text_split[:120], huggingface_embedding)\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = db1.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# Initialize the HuggingFaceHub model\n",
    "hf = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_length\": 500}\n",
    ")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"\n",
    "Use the following piece of context to answer the question asked.\n",
    "Please try to provide the answer only based on the context\n",
    "\n",
    "{context}\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answers:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# Initialize the DataAnalyzer\n",
    "analyzer = DataAnalyzer(df, llm=hf)  # Use hf instead of retrievalQA\n",
    "\n",
    "# Define the query\n",
    "query = analyzer.analysis_data()\n",
    "\n",
    "# Call the QA chain with our query\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Result:\n",
      "\n",
      "Use the following piece of context to answer the question asked.\n",
      "Please try to provide the answer only based on the context.\n",
      "\n",
      "region_id: 105 | sales_district: Victoria | sales_region: Canada West\n",
      "\n",
      "region_id: 42 | sales_district: San Francisco | sales_region: Central West\n",
      "\n",
      "region_id: 31 | sales_district: San Francisco | sales_region: Central West\n",
      "Question: Analyze this dataset based on statistical trends and patterns.\n",
      "\n",
      "Helpful Answers:\n",
      "- The sales_district \"San Francisco\" appears twice, both times in the sales_region \"Central West\".\n",
      "- The sales_region \"Canada West\" appears only once, associated with the sales_district \"Victoria\".\n",
      "- The sales_region \"Central West\" appears twice, associated with the sales_district \"San Francisco\" both times.\n",
      "- There are no duplicate region_id values in the dataset.\n",
      "- The dataset contains 3 unique region_id values, 2 unique sales_district values, and 2 unique sales_region values.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ÿ™ÿ£ŸÉÿØ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ÿßŸÑÿ™ŸàŸÉŸÜ ÿßŸÑÿÆÿßÿµ ÿ®ŸÉ ÿπŸÑŸâ Hugging Face\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"\n",
    "\n",
    "# ÿ™ÿ≠ŸÖŸäŸÑ ÿ®ŸäÿßŸÜÿßÿ™ CSV\n",
    "file_path = \"Regions.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ÿ•ŸÜÿ¥ÿßÿ° ŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ŸÖŸÜ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n",
    "documents = [\n",
    "    Document(page_content=\" | \".join([f\"{col}: {str(row[col])}\" for col in df.columns]))\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÜÿµŸàÿµ ÿ•ŸÑŸâ ÿ£ÿ¨ÿ≤ÿßÿ° ÿµÿ∫Ÿäÿ±ÿ©\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text_split = text_splitter.split_documents(documents)\n",
    "\n",
    "# ÿ™ŸáŸäÿ¶ÿ© ŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ™ÿ∂ŸÖŸäŸÜ\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# ÿ•ŸÜÿ¥ÿßÿ° ŸÇÿßÿπÿØÿ© ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ™ÿ¨Ÿáÿßÿ™\n",
    "vector_db = FAISS.from_documents(text_split, embedding_model)\n",
    "\n",
    "# ÿ•ÿπÿØÿßÿØ ŸÖÿ≥ÿ™ÿ±ÿ¨ÿπ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™\n",
    "retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# ÿ•ÿπÿØÿßÿØ ŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑŸÑÿ∫ÿ© ŸÖŸÜ Hugging Face\n",
    "hf = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_length\": 500}\n",
    ")\n",
    "\n",
    "# ÿ•ÿπÿØÿßÿØ ŸÇÿßŸÑÿ® ÿßŸÑÿ™Ÿàÿ¨ŸäŸá\n",
    "prompt_template = \"\"\"\n",
    "Use the following piece of context to answer the question asked.\n",
    "Please try to provide the answer only based on the context.\n",
    "\n",
    "{context}\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answers:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# ÿ•ÿπÿØÿßÿØ ÿ≥ŸÑÿ≥ŸÑÿ© ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ŸàÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# ÿ™ÿ≠ÿØŸäÿØ ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n",
    "query = \"Analyze this dataset based on statistical trends and patterns.\"\n",
    "\n",
    "# ÿ™ŸÜŸÅŸäÿ∞ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ\n",
    "result = retrievalQA.invoke(query)\n",
    "\n",
    "# ÿ∑ÿ®ÿßÿπÿ© ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©\n",
    "print(\"Analysis Result:\")\n",
    "print(result.get('result', 'No result found'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Temp\\ipykernel_14784\\2695014261.py:30: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding_model = OllamaEmbeddings(model=\"llama2\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API HTTP code: 500, {\"error\":{}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m documents \u001b[38;5;241m=\u001b[39m create_documents_from_rules(rules)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# ÿ™ÿØÿ±Ÿäÿ® ŸÜŸÖŸàÿ∞ÿ¨ RAG\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m retrievalQA, llm \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rag_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# ÿ™ÿ≠ŸÖŸäŸÑ ÿ®ŸäÿßŸÜÿßÿ™ CSV\u001b[39;00m\n\u001b[0;32m     64\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMy-Githup\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAxiora\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRegions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m, in \u001b[0;36mtrain_rag_system\u001b[1;34m(documents)\u001b[0m\n\u001b[0;32m     28\u001b[0m texts \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[0;32m     30\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m OllamaEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m vector_db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vector_db\u001b[38;5;241m.\u001b[39mas_retriever(search_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m, search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m})\n\u001b[0;32m     34\u001b[0m llm \u001b[38;5;241m=\u001b[39m Ollama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama2\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÜŸÖŸàÿ∞ÿ¨ Ollama\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:843\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[0;32m    841\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m--> 843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1041\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1022\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \n\u001b[0;32m   1025\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1041\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m   1043\u001b[0m         texts,\n\u001b[0;32m   1044\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1049\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\embeddings\\ollama.py:214\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed documents using an Ollama deployed embedding model.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m instruction_pairs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m--> 214\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction_pairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\embeddings\\ollama.py:202\u001b[0m, in \u001b[0;36mOllamaEmbeddings._embed\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_emb_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\embeddings\\ollama.py:176\u001b[0m, in \u001b[0;36mOllamaEmbeddings._process_emb_response\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference API HTTP code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;241m%\u001b[39m (res\u001b[38;5;241m.\u001b[39mstatus_code, res\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m    179\u001b[0m     )\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     t \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[1;31mValueError\u001b[0m: Error raised by inference API HTTP code: 500, {\"error\":{}}"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF ŸÑŸÇÿ±ÿßÿ°ÿ© ŸÖŸÑŸÅÿßÿ™ PDF\n",
    "import os  # ŸÑŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ÿßŸÑŸÖŸÑŸÅ\n",
    "from DataAnalyzer import DataAnalyzer  # ÿßÿ≥ÿ™Ÿäÿ±ÿßÿØ ÿßŸÑŸÉŸÑÿßÿ≥ ŸÖŸÜ ÿßŸÑŸÖŸÑŸÅ ÿßŸÑÿ¢ÿÆÿ±\n",
    "\n",
    "# 1. ÿ™ÿ≠ŸÖŸäŸÑ ŸÖŸÑŸÅ ÿßŸÑŸÇŸàÿßÿπÿØ (PDF) ŸÖŸÜ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    rules = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        rules += page.get_text()\n",
    "    return rules\n",
    "\n",
    "# 2. ÿ•ŸÜÿ¥ÿßÿ° Ÿàÿ´ÿßÿ¶ŸÇ (Documents) ŸÖŸÜ ÿßŸÑŸÇŸàÿßÿπÿØ\n",
    "def create_documents_from_rules(rules):\n",
    "    return [Document(page_content=rules)]\n",
    "\n",
    "# 3. ÿ™ÿØÿ±Ÿäÿ® ŸÜÿ∏ÿßŸÖ RAG ÿπŸÑŸâ ÿßŸÑŸÇŸàÿßÿπÿØ\n",
    "def train_rag_system(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "    vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "    \n",
    "    retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    llm = Ollama(model=\"llama2\")  # ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÜŸÖŸàÿ∞ÿ¨ Ollama\n",
    "    \n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return retrievalQA, llm  # ÿ•ÿ±ÿ¨ÿßÿπ retriever + llm ŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖŸá ŸÅŸä DataAnalyzer\n",
    "\n",
    "# 4. ÿ™ÿ≠ŸÖŸäŸÑ ŸÖŸÑŸÅ CSV\n",
    "def load_csv(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# ÿßŸÑÿÆÿ∑Ÿàÿßÿ™ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©\n",
    "if __name__ == \"__main__\":\n",
    "    # ÿ™ÿ≠ŸÖŸäŸÑ ŸÖŸÑŸÅ ÿßŸÑŸÇŸàÿßÿπÿØ (PDF) Ÿàÿ™ÿ≠ŸÑŸäŸÑŸá\n",
    "    with open(\"storying.pdf\", \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "    \n",
    "    rules = load_analysis_rules_from_memory(pdf_content)\n",
    "    documents = create_documents_from_rules(rules)\n",
    "    \n",
    "    # ÿ™ÿØÿ±Ÿäÿ® ŸÜŸÖŸàÿ∞ÿ¨ RAG\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "    \n",
    "    # ÿ™ÿ≠ŸÖŸäŸÑ ÿ®ŸäÿßŸÜÿßÿ™ CSV\n",
    "    csv_path = r\"D:\\My-Githup\\Axiora\\agent\\Regions.csv\"\n",
    "    try:\n",
    "        df = load_csv(csv_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        exit(1)\n",
    "    \n",
    "    # ÿ•ŸÜÿ¥ÿßÿ° ŸÉÿßÿ¶ŸÜ `DataAnalyzer`\n",
    "    analyzer = DataAnalyzer(df, llm=llm)  # ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ LLM ŸÅŸä ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ\n",
    "\n",
    "    # ÿ™ŸÜŸÅŸäÿ∞ `data_analysis` ŸÖŸÜ DataAnalyzer\n",
    "    query = analyzer.analysis_data()\n",
    "\n",
    "    # ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ RetrievalQA ŸÑŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ ÿßŸÑÿßÿ≥ÿ™ŸÅÿ≥ÿßÿ±\n",
    "    result = retrievalQA.invoke({\"query\": query})\n",
    "    \n",
    "    # ÿπÿ±ÿ∂ ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©\n",
    "    print(\"Final Analysis Result:\")\n",
    "    print(result['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõ†Ô∏è Generating new embeddings and saving FAISS index...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API HTTP code: 500, {\"error\":\"timed out waiting for llama runner to start - progress 0.49 - \"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m documents \u001b[38;5;241m=\u001b[39m create_documents_from_rules(rules)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# ‚úÖ ÿ™ÿØÿ±Ÿäÿ® ŸÜŸÖŸàÿ∞ÿ¨ RAG ÿ£Ÿà ÿ™ÿ≠ŸÖŸäŸÑ FAISS\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m retrievalQA, llm \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rag_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# ‚úÖ ÿ™ÿ≠ŸÖŸäŸÑ ÿ®ŸäÿßŸÜÿßÿ™ CSV\u001b[39;00m\n\u001b[0;32m     92\u001b[0m df \u001b[38;5;241m=\u001b[39m load_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n",
      "Cell \u001b[1;32mIn[5], line 38\u001b[0m, in \u001b[0;36mtrain_rag_system\u001b[1;34m(documents)\u001b[0m\n\u001b[0;32m     35\u001b[0m texts \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[0;32m     37\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m OllamaEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m vector_db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# üî• ÿ≠ŸÅÿ∏ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÑŸÑŸÖÿ±ÿßÿ™ ÿßŸÑŸÇÿßÿØŸÖÿ©\u001b[39;00m\n\u001b[0;32m     41\u001b[0m vector_db\u001b[38;5;241m.\u001b[39msave_local(FAISS_DB_PATH)\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:843\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[0;32m    841\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m--> 843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1041\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1022\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \n\u001b[0;32m   1025\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1041\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m   1043\u001b[0m         texts,\n\u001b[0;32m   1044\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1049\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\embeddings\\ollama.py:214\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed documents using an Ollama deployed embedding model.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m instruction_pairs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m--> 214\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction_pairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\embeddings\\ollama.py:202\u001b[0m, in \u001b[0;36mOllamaEmbeddings._embed\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_emb_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\embeddings\\ollama.py:176\u001b[0m, in \u001b[0;36mOllamaEmbeddings._process_emb_response\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference API HTTP code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;241m%\u001b[39m (res\u001b[38;5;241m.\u001b[39mstatus_code, res\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m    179\u001b[0m     )\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     t \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[1;31mValueError\u001b[0m: Error raised by inference API HTTP code: 500, {\"error\":\"timed out waiting for llama runner to start - progress 0.49 - \"}"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF ŸÑŸÇÿ±ÿßÿ°ÿ© ŸÖŸÑŸÅÿßÿ™ PDF\n",
    "import os  # ŸÑŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ÿßŸÑŸÖŸÑŸÅ\n",
    "from DataAnalyzer import DataAnalyzer  # ÿßÿ≥ÿ™Ÿäÿ±ÿßÿØ ÿßŸÑŸÉŸÑÿßÿ≥ ŸÖŸÜ ÿßŸÑŸÖŸÑŸÅ ÿßŸÑÿ¢ÿÆÿ±\n",
    "\n",
    "# üîπ ŸÖÿ≥ÿßÿ± ÿ™ÿÆÿ≤ŸäŸÜ FAISS ŸÑÿ™ÿ¨ŸÜÿ® ÿ•ÿπÿßÿØÿ© ÿ≠ÿ≥ÿßÿ® embeddings\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1Ô∏è‚É£ ÿ™ÿ≠ŸÖŸäŸÑ ŸÖŸÑŸÅ ÿßŸÑŸÇŸàÿßÿπÿØ (PDF) ŸÖŸÜ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    rules = \"\\n\".join(page.get_text() for page in doc)\n",
    "    return rules\n",
    "\n",
    "# 2Ô∏è‚É£ ÿ•ŸÜÿ¥ÿßÿ° Ÿàÿ´ÿßÿ¶ŸÇ (Documents) ŸÖŸÜ ÿßŸÑŸÇŸàÿßÿπÿØ\n",
    "def create_documents_from_rules(rules):\n",
    "    return [Document(page_content=rules)]\n",
    "\n",
    "# 3Ô∏è‚É£ ÿ™ÿØÿ±Ÿäÿ® ÿ£Ÿà ÿ™ÿ≠ŸÖŸäŸÑ ŸÜŸÖŸàÿ∞ÿ¨ RAG ŸÖÿπ FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"ÿ™ÿØÿ±Ÿäÿ® ÿ£Ÿà ÿ™ÿ≠ŸÖŸäŸÑ ŸÜŸÖŸàÿ∞ÿ¨ RAG ŸÖÿπ FAISS ŸÑÿ™ÿ¨ŸÜÿ® ÿ•ÿπÿßÿØÿ© ÿ≠ÿ≥ÿßÿ® embeddings\"\"\"\n",
    "    \n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nüîÑ Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(FAISS_DB_PATH, OllamaEmbeddings(model=\"llama2\"))\n",
    "    else:\n",
    "        print(\"\\nüõ†Ô∏è Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        \n",
    "        embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        \n",
    "        # üî• ÿ≠ŸÅÿ∏ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÑŸÑŸÖÿ±ÿßÿ™ ÿßŸÑŸÇÿßÿØŸÖÿ©\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "    \n",
    "    # üîπ ÿ™ÿ≠ŸàŸäŸÑ FAISS ÿ•ŸÑŸâ `retriever`\n",
    "    retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    \n",
    "    # üîπ ÿ•ŸÜÿ¥ÿßÿ° ŸÜŸÖŸàÿ∞ÿ¨ LLM\n",
    "    llm = Ollama(model=\"llama2\")\n",
    "    \n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return retrievalQA, llm\n",
    "\n",
    "# 4Ô∏è‚É£ ÿ™ÿ≠ŸÖŸäŸÑ ŸÖŸÑŸÅ CSV\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# 5Ô∏è‚É£ ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ DataAnalyzer Ÿà RAG\n",
    "def analyze_data_with_rag(analyzer, retrievalQA, df):\n",
    "    \"\"\"ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ DataAnalyzer ÿ´ŸÖ ÿ™ÿ≠ÿ≥ŸäŸÜŸáÿß ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ RAG\"\"\"\n",
    "    \n",
    "    query = analyzer.analysis_data()  # ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ DataAnalyzer\n",
    "    \n",
    "    # ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ RAG\n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        data_row = \" | \".join([f\"{col}: {str(row[col])}\" for col in df.columns])\n",
    "        final_query = f\"{query} \\n\\nAnalyze this data based on the rules: {data_row}\"\n",
    "        \n",
    "        result = retrievalQA.invoke({\"query\": final_query})\n",
    "        results.append(result['result'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# üéØ ÿßŸÑÿÆÿ∑Ÿàÿßÿ™ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©\n",
    "if __name__ == \"__main__\":\n",
    "    # ‚úÖ ÿ™ÿ≠ŸÖŸäŸÑ ŸÖŸÑŸÅ ÿßŸÑŸÇŸàÿßÿπÿØ (PDF) Ÿàÿ™ÿ≠ŸÑŸäŸÑŸá\n",
    "    with open(\"storying.pdf\", \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "    \n",
    "    rules = load_analysis_rules_from_memory(pdf_content)\n",
    "    documents = create_documents_from_rules(rules)\n",
    "    \n",
    "    # ‚úÖ ÿ™ÿØÿ±Ÿäÿ® ŸÜŸÖŸàÿ∞ÿ¨ RAG ÿ£Ÿà ÿ™ÿ≠ŸÖŸäŸÑ FAISS\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "    \n",
    "    # ‚úÖ ÿ™ÿ≠ŸÖŸäŸÑ ÿ®ŸäÿßŸÜÿßÿ™ CSV\n",
    "    df = load_csv(\"Regions.csv\")  \n",
    "\n",
    "    # ‚úÖ ÿ•ŸÜÿ¥ÿßÿ° ŸÉÿßÿ¶ŸÜ `DataAnalyzer`\n",
    "    analyzer = DataAnalyzer(df, llm=llm)  \n",
    "\n",
    "    # ‚úÖ ÿ™ŸÜŸÅŸäÿ∞ `data_analysis` Ÿàÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ RAG\n",
    "    analysis_results = analyze_data_with_rag(analyzer, retrievalQA, df)\n",
    "\n",
    "    # ‚úÖ ÿπÿ±ÿ∂ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨\n",
    "    for i, result in enumerate(analysis_results):\n",
    "        print(f\"\\nüîç Analysis for row {i+1}:\")\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Temp\\ipykernel_14784\\2324853806.py:34: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama2\")  # ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÜŸÖŸàÿ∞ÿ¨ Ollama\n",
      "d:\\My-Githup\\Axiora\\agent\\huggingface\\DataAnalyzer.py:35: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  analysis_chain = LLMChain(llm=self.llm, prompt=analysis_template)\n",
      "d:\\My-Githup\\Axiora\\agent\\huggingface\\DataAnalyzer.py:38: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  analysis = analysis_chain.run(data_info=data_info)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Analysis Result:\n",
      "Based on the information provided in the question, here are some possible answers to the recommended actions for the organization:\n",
      "\n",
      "1. Regionalization of Strategies: Develop regional-specific marketing strategies to maximize revenue potential. This could involve tailoring products or services to meet the unique needs of each region, as well as investing in local marketing campaigns and partnerships.\n",
      "2. Segmentation of Sales Districts: Segment sales districts based on their characteristics (e.g., demographics, psychographics) to better understand customer preferences and behaviors. This could involve creating customer personas or segments and developing targeted marketing strategies for each segment.\n",
      "3. Investigation of Outliers: Investigate the outliers in the sales region data to identify specific marketing strategies or customer segments that are driving revenue growth. This could involve conducting market research or analyzing customer data to understand their behaviors and preferences.\n",
      "4. Improved Data Quality: Implement data quality measures to ensure accurate analysis and decision-making. This could involve standardizing data collection and validation processes, as well as implementing data cleaning and filtering techniques.\n",
      "5. Customer Segmentation: Analyze customer segments based on demographics, psychographics, or purchase history to identify new marketing opportunities and improve customer targeting. This could involve creating customer personas or segments and developing targeted marketing strategies for each segment.\n",
      "6. Performance Metrics Development: Develop custom performance metrics that align with the organization's goals and objectives to measure success and track progress over time. This could involve defining key performance indicators (KPIs) and tracking them regularly to evaluate the effectiveness of marketing strategies.\n",
      "7. Data-Driven Decision-Making: Utilize data visualization tools and techniques to communicate insights and recommendations to stakeholders, ensuring that data-driven decision-making is a central part of the organization's strategy. This could involve creating interactive dashboards or reports to help stakeholders understand complex data and make informed decisions.\n"
     ]
    }
   ],
   "source": [
    "## importance\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF ŸÑŸÇÿ±ÿßÿ°ÿ© ŸÖŸÑŸÅÿßÿ™ PDF\n",
    "import os  # ŸÑŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ÿßŸÑŸÖŸÑŸÅ\n",
    "from DataAnalyzer import DataAnalyzer  # ÿßÿ≥ÿ™Ÿäÿ±ÿßÿØ ÿßŸÑŸÉŸÑÿßÿ≥ ŸÖŸÜ ÿßŸÑŸÖŸÑŸÅ ÿßŸÑÿ¢ÿÆÿ±\n",
    "\n",
    "# 1. ÿ™ÿ≠ŸÖŸäŸÑ ŸÖŸÑŸÅ ÿßŸÑŸÇŸàÿßÿπÿØ (PDF) ŸÖŸÜ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    rules = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        rules += page.get_text()\n",
    "    return rules\n",
    "\n",
    "# 2. ÿ•ŸÜÿ¥ÿßÿ° Ÿàÿ´ÿßÿ¶ŸÇ (Documents) ŸÖŸÜ ÿßŸÑŸÇŸàÿßÿπÿØ\n",
    "def create_documents_from_rules(rules):\n",
    "    return [Document(page_content=rules)]\n",
    "\n",
    "# 3. ÿ™ÿØÿ±Ÿäÿ® ŸÜÿ∏ÿßŸÖ RAG ÿπŸÑŸâ ÿßŸÑŸÇŸàÿßÿπÿØ\n",
    "def train_rag_system(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "    vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "    \n",
    "    retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    llm = Ollama(model=\"llama2\")  # ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÜŸÖŸàÿ∞ÿ¨ Ollama\n",
    "    \n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return retrievalQA, llm  # ÿ•ÿ±ÿ¨ÿßÿπ retriever + llm ŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖŸá ŸÅŸä DataAnalyzer\n",
    "\n",
    "# 4. ÿ™ÿ≠ŸÖŸäŸÑ ŸÖŸÑŸÅ CSV\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# ÿßŸÑÿÆÿ∑Ÿàÿßÿ™ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©\n",
    "if __name__ == \"__main__\":\n",
    "    # ÿ™ÿ≠ŸÖŸäŸÑ ŸÖŸÑŸÅ ÿßŸÑŸÇŸàÿßÿπÿØ (PDF) Ÿàÿ™ÿ≠ŸÑŸäŸÑŸá\n",
    "    with open(\"storying.pdf\", \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "    \n",
    "    rules = load_analysis_rules_from_memory(pdf_content)\n",
    "    documents = create_documents_from_rules(rules)\n",
    "    \n",
    "    # ÿ™ÿØÿ±Ÿäÿ® ŸÜŸÖŸàÿ∞ÿ¨ RAG\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "    \n",
    "    # ÿ™ÿ≠ŸÖŸäŸÑ ÿ®ŸäÿßŸÜÿßÿ™ CSV ÿ®ÿØŸàŸÜ ÿßŸÑÿ≠ÿßÿ¨ÿ© ŸÑŸÖÿ≥ÿßÿ± ŸÉÿßŸÖŸÑ\n",
    "    df = load_csv(\"Regions.csv\")  \n",
    "\n",
    "    # ÿ•ŸÜÿ¥ÿßÿ° ŸÉÿßÿ¶ŸÜ `DataAnalyzer`\n",
    "    analyzer = DataAnalyzer(df, llm=llm)  # ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ LLM ŸÅŸä ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ\n",
    "\n",
    "    # ÿ™ŸÜŸÅŸäÿ∞ `data_analysis` ŸÖŸÜ DataAnalyzer\n",
    "    query = analyzer.analysis_data()\n",
    "\n",
    "    # ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ RetrievalQA ŸÑŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ ÿßŸÑÿßÿ≥ÿ™ŸÅÿ≥ÿßÿ±\n",
    "    result = retrievalQA.invoke({\"query\": query})\n",
    "    \n",
    "    # ÿπÿ±ÿ∂ ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©\n",
    "    print(\"Final Analysis Result:\")\n",
    "    print(result['result'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
